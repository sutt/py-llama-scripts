### Todos

[x] remove the initial printout
[x] print the multiple times
[x] explain why the thing stopped
    -> it didn't but the following completion took all the tokens
    \end{blockquote}
    -> fixed via replace the apostophes in the quoted article
[x] different seed, temp
[x] separate into more modules
[x] better/DRYer time-logging
[x] add typing
[x] add random/setseed
[x] add mini example
    - e.g. "the mouse was named Dave" "Q: what is mouse's name?"
[x] add line carriage between completion and 
[x] move prompt into data directory
[x] refactor askTheQuestion / createState

[X] refactor out stderr_redirect from main
[x] refactor as much of llama_log_set boilerplate from main as possible
[ ] add llm stats / token length stats to ProfileStats
    - completion time depends on length
[ ] json print rounded floats

### Thoughts:

