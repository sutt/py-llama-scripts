{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More-Context-1\n",
    "\n",
    "Create more context length and measure the question length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import json\n",
    "from llama_cpp import llama_log_set\n",
    "from llama_cpp import Llama\n",
    "\n",
    "def my_log_callback(level, message, user_data):\n",
    "    pass\n",
    "log_callback = ctypes.CFUNCTYPE(None, ctypes.c_int, ctypes.c_char_p, ctypes.c_void_p)(my_log_callback)\n",
    "llama_log_set(log_callback, ctypes.c_void_p())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the following, answer the multiple choice question. Say nothing other than the answer. Only use the possible answers given, e.g. if the only answers are \"A) True B) False\", then only say either \"A) True\" or \"B) False\". Or, e.g. if the choices are A-D then only say e.g. \"C) Napolean Bonaparte\". Don\\'t add any text to the beginning or end of the answer.\\n\\nBelow is data from a Wordle game.\\n\\nIn the data, one portion is marked missing, indicated by the ??? tokens. \\n\\nRespond to the multiple choice question by selecting the answer which best completes the missing portion of the data.\\n[\\n  {\\n    \"word\": \"ascot\",\\n    \"current_row\": 0,\\n    \"win\": false,\\n    \"results\": [\\n      \"absent\",\\n      \"present\",\\n      \"absent\",\\n      \"absent\",\\n      \"correct\"\\n    ]\\n  },\\n  {\\n    \"word\": \"egret\",\\n    \"current_row\": 1,\\n    \"win\": false,\\n    \"results\": [\\n      \"absent\",\\n      \"absent\",\\n      \"present\",\\n      \"absent\",\\n      \"correct\"\\n    ]\\n  },\\n  {\\n    \"word\": \"greed\",\\n    \"current_row\": 2,\\n    \"win\": false,\\n    \"results\": [\\n      \"absent\",\\n      \"present\",\\n      \"absent\",\\n      \"absent\",\\n      \"absent\"\\n    ]\\n  },\\n  {\\n    \"word\": \"cower\",\\n    \"current_row\": 3,\\n    \"win\": false,\\n    \"results\": \"???\"\\n  },\\n  {\\n    \"word\": \"cabby\",\\n    \"current_row\": 4,\\n    \"win\": false,\\n    \"results\": [\\n      \"absent\",\\n      \"absent\",\\n      \"absent\",\\n      \"absent\",\\n      \"absent\"\\n    ]\\n  },\\n  {\\n    \"word\": \"shirt\",\\n    \"current_row\": 5,\\n    \"win\": true,\\n    \"results\": [\\n      \"correct\",\\n      \"correct\",\\n      \"correct\",\\n      \"correct\",\\n      \"correct\"\\n    ]\\n  }\\n]\\n\\nPossible Answers:\\nA) [\\'absent\\', \\'absent\\', \\'absent\\', \\'absent\\', \\'present\\']\\nB) [\\'correct\\', \\'absent\\', \\'absent\\', \\'absent\\', \\'present\\']\\nC) [\\'absent\\', \\'absent\\', \\'present\\', \\'absent\\', \\'absent\\']\\nD) [\\'absent\\', \\'absent\\', \\'absent\\', \\'correct\\', \\'present\\']'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load(open('ctx1.json', 'r'))\n",
    "preamble = '''In the following, answer the multiple choice question. Say nothing other than the answer. Only use the possible answers given, e.g. if the only answers are \"A) True B) False\", then only say either \"A) True\" or \"B) False\". Or, e.g. if the choices are A-D then only say e.g. \"C) Napolean Bonaparte\". Don't add any text to the beginning or end of the answer.'''\n",
    "question = preamble + '\\n\\n' + data['question']\n",
    "question = question.rstrip()\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# model_fn = '/mnt/disk2/llamas/TheBloke7B/llama-2-7b.Q4_K_M.gguf'\n",
    "model_fn = '/mnt/disk2/llamas/TheBlokeChat13B/llama-2-13b-chat.Q4_K_S.gguf'\n",
    "llm = Llama(\n",
    "    model_fn, \n",
    "    n_ctx=512*2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-734db6b2-5ebd-4e12-b9ab-9005320936a7',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1704342421,\n",
       " 'model': '/mnt/disk2/llamas/TheBlokeChat13B/llama-2-13b-chat.Q4_K_S.gguf',\n",
       " 'choices': [{'text': ' features were',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 12, 'completion_tokens': 2, 'total_tokens': 14}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = '''She was the most beautiful woman in the world. Her'''\n",
    "prompt = prompt.rstrip()\n",
    "# num_logprobs =  40\n",
    "model_params = {\n",
    "    'temperature': 0.8,\n",
    "    'max_tokens': 2,\n",
    "}\n",
    "output = llm(\n",
    "    prompt=prompt, \n",
    "    # logprobs=num_logprobs, \n",
    "    **model_params,\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_tokens = llm.tokenize(question.encode('utf-8'))\n",
    "len(q_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-a7e38302-1257-44f4-a8a2-b7c408bc2dbd',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1704342443,\n",
       " 'model': '/mnt/disk2/llamas/TheBlokeChat13B/llama-2-13b-chat.Q4_K_S.gguf',\n",
       " 'choices': [{'text': '\\n\\n',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 640, 'completion_tokens': 2, 'total_tokens': 642}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = question\n",
    "prompt = prompt.rstrip()\n",
    "model_params = {\n",
    "    'temperature': 0.8,\n",
    "    'max_tokens': 2,\n",
    "}\n",
    "output = llm(\n",
    "    prompt=prompt, \n",
    "    **model_params,\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only 3 tokens a second\n",
    "# Short Run: 5s | 15 tokens -> 3tokens/s\n",
    "# Long Run: 4m16s (256s) | 640 tokens -> 3 tokens/s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
